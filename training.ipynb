{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e615dc8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from metrics.english_metrics import get_features\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68d844",
   "metadata": {},
   "source": [
    "## Getting Data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wikipedia_helper import get_text_from_wikipedia\n",
    "from preprocessing.indic_preprocessor import preprocess_text\n",
    "from translation.indic_en_translation import translate_indic_to_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code = \"mr\"\n",
    "text = get_text_from_wikipedia(\"à¤µà¤¿à¤°à¤¾à¤Ÿ à¤•à¥‹à¤¹à¤²à¥€\", lang_code)\n",
    "sentences = preprocess_text(text, \"mar_Deva\")\n",
    "\n",
    "# Translate sentences to English in batches\n",
    "result = []\n",
    "BATCH_SIZE = 10\n",
    "for i in range(0, len(sentences), BATCH_SIZE):\n",
    "    batch_sentences = sentences[i:i + BATCH_SIZE]\n",
    "    result.extend(translate_indic_to_english([{\"sentence\": sentence} for sentence in batch_sentences], lang_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e322607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results into csv\n",
    "translated_df = pd.DataFrame(result)\n",
    "translated_df.to_csv(\"models\\\\wikipedia_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44a94d",
   "metadata": {},
   "source": [
    "## Loading Wikipedia Data and getting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = \"models\\\\wikipedia_data.csv\"\n",
    "data = pd.read_csv(CSV_FILE)\n",
    "\n",
    "detailed_data = get_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the detailed data to csv\n",
    "detailed_data.to_csv(\"models\\\\final_detailed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809d15",
   "metadata": {},
   "source": [
    "## Active Learning on partial annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the partial detailed data\n",
    "partial_annotated_data = pd.read_csv(\"models\\\\partial_annotation.csv\")\n",
    "partial_annotated_data = get_features(partial_annotated_data)\n",
    "\n",
    "# Function to plot feature vs difficulty (scatter plot and line)\n",
    "def plot_feature_vs_difficulty(feature_name: str):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=partial_annotated_data[feature_name], y=partial_annotated_data['difficulty'])\n",
    "    plt.title(f'Scatter Plot of {feature_name} vs Difficulty')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('difficulty')\n",
    "\n",
    "    # Line plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=partial_annotated_data[feature_name], y=partial_annotated_data['difficulty'])\n",
    "    plt.title(f'Line Plot of {feature_name} vs Difficulty')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('difficulty')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Keep different features on x axis and difficulty on y axis\n",
    "# features_to_plot = ['is_passive', 'subordinate_clause_count', 'noun_phrase_count', 'average_noun_phrase_length', 'prepositional_phrase_count', 'verb_phrase_count', 'adjective_count', 'adverb_count', 'proper_noun_count', 'sentence_length', 'avg_rh_score']\n",
    "# for feature in features_to_plot:\n",
    "#     # Line graph and scatter plot side by side\n",
    "#     plot_feature_vs_difficulty(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from modAL.models import ActiveLearner\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- CUSTOM QUERY STRATEGY ---\n",
    "def random_forest_variance_sampling(learner, X_pool):\n",
    "    all_preds = np.array([\n",
    "        tree.predict(X_pool) for tree in learner.estimator.estimators_\n",
    "    ])\n",
    "    variances = np.std(all_preds, axis=0)\n",
    "    query_idx = np.argmax(variances)\n",
    "    return np.array([query_idx]), X_pool[query_idx].reshape(1, -1)\n",
    "\n",
    "\n",
    "# ----- PREPARE DATA -----\n",
    "labeled_data = partial_annotated_data[partial_annotated_data['difficulty'].notnull()].copy()\n",
    "unlabeled_data = partial_annotated_data[partial_annotated_data['difficulty'].isnull()].copy()\n",
    "\n",
    "X_initial = labeled_data.drop(columns=['difficulty', 'sentence', 'translation']).values\n",
    "y_initial = labeled_data['difficulty'].values\n",
    "\n",
    "X_pool = unlabeled_data.drop(columns=['difficulty', 'sentence', 'translation']).values\n",
    "\n",
    "\n",
    "# ----- ACTIVE LEARNER -----\n",
    "learner = ActiveLearner(\n",
    "    estimator=RandomForestRegressor(n_estimators=50),\n",
    "    X_training=X_initial,\n",
    "    y_training=y_initial,\n",
    "    query_strategy=random_forest_variance_sampling\n",
    ")\n",
    "\n",
    "\n",
    "# ----- AUTOSAVE FUNCTION -----\n",
    "def save_progress():\n",
    "    \"\"\"Save current progress back into a single CSV.\"\"\"\n",
    "    combined = pd.concat([labeled_data, unlabeled_data], ignore_index=True)\n",
    "    combined.to_csv(\"models\\\\annotated_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# ----- ACTIVE LEARNING LOOP -----\n",
    "while len(unlabeled_data) > 0:\n",
    "\n",
    "    # Query next sample\n",
    "    query_idx, query_instance = learner.query(X_pool)\n",
    "    q = query_idx[0]\n",
    "\n",
    "    print(f\"\\nðŸ“Š Progress: Labeled = {len(labeled_data)}, Unlabeled = {len(unlabeled_data)}, Total = {len(labeled_data) + len(unlabeled_data)}\")\n",
    "\n",
    "    print(\"\\nPlease annotate the following sentence:\")\n",
    "    print(unlabeled_data.iloc[q]['sentence'])\n",
    "    print(\"Translation:\")\n",
    "    print(unlabeled_data.iloc[q]['translation'])\n",
    "\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter difficulty score (1-10) or 'q' to quit: \").strip()\n",
    "\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"\\nðŸ”¹ Quitting early...\")\n",
    "        save_progress()\n",
    "        break\n",
    "    elif not user_input.replace('.', '', 1).isdigit() or not (1 <= float(user_input) <= 10):\n",
    "        print(\"â— Invalid input. Please enter a number between 1 and 10.\")\n",
    "        continue\n",
    "\n",
    "    difficulty = float(user_input)\n",
    "\n",
    "    # Teach the model\n",
    "    learner.teach(X_pool[q].reshape(1, -1), [difficulty])\n",
    "\n",
    "    # Store annotation\n",
    "    unlabeled_data.loc[q, \"difficulty\"] = difficulty\n",
    "\n",
    "    # Move annotated row to labeled_data\n",
    "    labeled_data = pd.concat([labeled_data, unlabeled_data.loc[[q]]], ignore_index=True)\n",
    "\n",
    "    # Remove from active pool\n",
    "    X_pool = np.delete(X_pool, q, axis=0)\n",
    "    unlabeled_data = unlabeled_data.drop(unlabeled_data.index[q]).reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    # Runs only if loop finishes normally (no break)\n",
    "    print(\"\\nðŸŽ‰ Completed all annotation steps!\")\n",
    "    save_progress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934358ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save ONLY the Random Forest model\n",
    "joblib.dump(learner.estimator, 'models\\\\al_random_forest_model.pkl')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bc79a",
   "metadata": {},
   "source": [
    "## Trying Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5ae15",
   "metadata": {},
   "source": [
    "Step 1: Start the server\n",
    "    : mlflow server --port 8080\n",
    "\n",
    "Step 2: set tracking uri and experiment\n",
    "\n",
    "Step 3: visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Connect to remote MLflow server\n",
    "# Later shift these values to env\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")\n",
    "mlflow.set_experiment(\"my-genai-experiment\")\n",
    "print(\"âœ“ MLflow server configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "\n",
    "annotated_data = pd.read_csv(\"models\\\\annotated_data.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = annotated_data.drop(columns=['difficulty', 'sentence', 'translation'])\n",
    "y = annotated_data['difficulty']\n",
    "\n",
    "print(y.min(), y.max())\n",
    "# num_bins = 4\n",
    "# bins = np.linspace(y.min(), y.max(), num_bins + 1)\n",
    "\n",
    "# y_binned = np.digitize(y, bins) - 1  # Bins are 0-indexed\n",
    "y_binned = pd.qcut(y, q=10, duplicates='drop') \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y_binned, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # âœ… Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "    # âœ… Train model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # âœ… Predictions\n",
    "    rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # âœ… -------- PLOT 1: Predicted vs Actual --------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, rf_y_pred)\n",
    "    plt.plot([y_test.min(), y_test.max()],\n",
    "             [y_test.min(), y_test.max()],\n",
    "             'r--')\n",
    "    plt.xlabel(\"Actual difficulty\")\n",
    "    plt.ylabel(\"Predicted difficulty\")\n",
    "    plt.title(\"Predicted vs Actual difficulty\")\n",
    "\n",
    "    plt.savefig(\"pred_vs_actual.png\")\n",
    "    mlflow.log_artifact(\"pred_vs_actual.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # âœ… -------- PLOT 2: Confusion Matrix --------\n",
    "    cm = confusion_matrix(y_test, rf_y_pred.round())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # âœ… -------- METRICS --------\n",
    "    mse = mean_squared_error(y_test, rf_y_pred)\n",
    "    r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    print(f\"âœ… MSE: {mse}, R^2: {r2}\")\n",
    "\n",
    "    # âœ… -------- SAVE MODEL --------\n",
    "    mlflow.sklearn.log_model(rf_model, \"random_forest_model\")\n",
    "\n",
    "    mlflow.log_artifact(\"models\\\\annotated_data.csv\")\n",
    "\n",
    "print(\"âœ… All results logged to MLflow UI successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# plot graph of predicted vs actual with a line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual difficulty\")\n",
    "plt.ylabel(\"Predicted difficulty\")\n",
    "plt.title(\"Predicted vs Actual difficulty\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred.round())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate MSE and R^2\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, rf_y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual difficulty\")\n",
    "plt.ylabel(\"Predicted difficulty\")\n",
    "plt.title(\"Predicted vs Actual difficulty\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, rf_y_pred.round())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, rf_y_pred)\n",
    "r2 = r2_score(y_test, rf_y_pred)\n",
    "print(f\"MSE: {mse}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR Model\n",
    "from sklearn.svm import SVR\n",
    "svr_model = SVR(kernel='rbf')\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Visualize SVR results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, svr_y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual difficulty\")\n",
    "plt.ylabel(\"Predicted difficulty\")\n",
    "plt.title(\"Predicted vs Actual difficulty\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, svr_y_pred.round())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test, svr_y_pred)\n",
    "r2 = r2_score(y_test, svr_y_pred)\n",
    "print(f\"MSE: {mse}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c21915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training XGBoost Regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xg_reg = XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
    "                max_depth=5, alpha=10, n_estimators=100)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "preds = xg_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
